version: "3.8"
services:
  cuda_app:
    container_name: ai_toolkit_server
    build:
      context: ..
      dockerfile: server/Dockerfile
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    volumes:
      - ..:/app
      - /home/zetaphor/LLMs/:/models
    environment:
      USE_MLOCK: "0"
      HOST: "0.0.0.0"
      CUDA_DOCKER_ARCH: "all"
      GGML_CUDA: "1"
      MODEL_NAME: "phi3" # or "llama3"
      N_GPU_LAYERS: "-1"
    cap_add:
      - SYS_RESOURCE
    ports:
      - "8000:8000"
    entrypoint: ["/bin/bash", "-lc"]
    command: ["uvicorn server.api.main:app --host 0.0.0.0 --port 8000"]
